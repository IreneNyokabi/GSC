{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c554282",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import modules\n",
    "import searchconsole\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import httplib2\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import json\n",
    "logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)\n",
    "logging.getLogger('oauth2client._helpers').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from apiclient import errors\n",
    "from apiclient.discovery import build\n",
    "from oauth2client.client import OAuth2WebServerFlow\n",
    "from oauth2client.file import Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "222a7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get client-id and client-secret from credentials file from GCP - (https://console.cloud.google.com/)\n",
    "# Copy your credentials from the console\n",
    "CLIENT_ID = {'insert-here'}\n",
    "CLIENT_SECRET = {'insert-here'}\n",
    "\n",
    "# Check https://developers.google.com/webmaster-tools/search-console-api-original/v3/ for all available scopes\n",
    "OAUTH_SCOPE = 'https://www.googleapis.com/auth/webmasters.readonly'\n",
    "\n",
    "# Redirect URI for installed apps\n",
    "REDIRECT_URI = 'urn:ietf:wg:oauth:2.0:oob'\n",
    "\n",
    "# Create a credential storage object.  pick the file name, in your folder, this file will be generated. It will help with authentication\n",
    "storage = Storage('gsc_credentials')\n",
    "\n",
    "# Attempt to load existing credentials.  Null is returned if it fails.\n",
    "credentials = storage.get()\n",
    "\n",
    "# Only attempt to get new credentials if the load failed.\n",
    "if not credentials:\n",
    "\n",
    "    # Run through the OAuth flow and retrieve credentials\n",
    "    flow = OAuth2WebServerFlow(CLIENT_ID, CLIENT_SECRET, OAUTH_SCOPE, REDIRECT_URI)\n",
    "    authorize_url = flow.step1_get_authorize_url()\n",
    "    print ('Go to the following link in your browser: ' + authorize_url)\n",
    "    code = input('Enter verification code: ').strip()\n",
    "    credentials = flow.step2_exchange(code)\n",
    "    storage.put(credentials)\n",
    "    if storage.get():\n",
    "        print('Credentials saved for later.')\n",
    "\n",
    "# Create an httplib2.Http object and authorize it with our credentials\n",
    "http = httplib2.Http()\n",
    "http = credentials.authorize(http)\n",
    "\n",
    "webmasters_service = build('searchconsole', 'v1', http=http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web property\n",
    "#for my call, I am looking to get date and page dimensions, substitue with the correct dimension you'd want to pull\n",
    "query_params = {\"startDate\": \"2022-02-25\", \"endDate\": \"2022-03-05\",\"searchType\": \"web\",\"dimensions\":[\"date\",\"page\"]}\n",
    "try:\n",
    "    quered_results = webmasters_service.searchanalytics().query(\n",
    "        key=\"<KEY>\",\n",
    "        siteUrl={\"insert-web-uri\"},\n",
    "        body=query_params,\n",
    "        fields=\"rows\",\n",
    "        alt=\"json\"\n",
    "    ).execute()\n",
    "    print(quered_results)\n",
    "except googleapiclient.errors.HttpError as e:\n",
    "    print(e)\n",
    "    \n",
    "##pull data for different search_types\n",
    "web_report = pd.json_normalize(quered_results, record_path=['rows'],\n",
    "    errors='ignore')\n",
    "\n",
    "web_report['search_type'] = \"web\" \n",
    "\n",
    "## create a dataframe\n",
    "web_report = pd.DataFrame(data=web_report)\n",
    "\n",
    "# new df from the column of lists\n",
    "split_df = pd.DataFrame(web_report['keys'].tolist(), columns=['date', 'landingpage'])\n",
    "# display the resulting df\n",
    "\n",
    "\n",
    "#concat previious dataframe\n",
    "web_report = pd.concat([web_report, split_df], axis=1)\n",
    "\n",
    "# drop Values\n",
    "web_report = web_report.drop('keys', axis=1)\n",
    "web_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749010ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data for search type \"news\"\n",
    "query_params = {\"startDate\": \"2022-02-25\", \"endDate\": \"2022-03-05\",\"searchType\": \"news\",\"dimensions\":[\"date\",\"page\"]}\n",
    "try:\n",
    "    quered_results = webmasters_service.searchanalytics().query(\n",
    "        key=\"<KEY>\",\n",
    "        siteUrl={\"insert-web-uri\"},\n",
    "        body=query_params,\n",
    "        fields=\"rows\",\n",
    "        alt=\"json\"\n",
    "    ).execute()\n",
    "    print(quered_results)\n",
    "except googleapiclient.errors.HttpError as e:\n",
    "    print(e)\n",
    "    \n",
    "##pull data for different search_types\n",
    "news_report = pd.json_normalize(quered_results, record_path=['rows'],\n",
    "    errors='ignore')\n",
    "\n",
    "news_report['search_type'] = \"news\" \n",
    "\n",
    "## create a dataframe\n",
    "news_report = pd.DataFrame(data=news_report)\n",
    "\n",
    "# new df from the column of lists\n",
    "split_df = pd.DataFrame(news_report['keys'].tolist(), columns=['date', 'landingpage'])\n",
    "# display the resulting df\n",
    "\n",
    "\n",
    "#concat previious dataframe\n",
    "news_report = pd.concat([news_report, split_df], axis=1)\n",
    "\n",
    "# drop Values\n",
    "news_report = news_report.drop('keys', axis=1)\n",
    "   \n",
    "news_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for search type image\n",
    "query_params = {\"startDate\": \"2022-02-25\", \"endDate\": \"2022-03-05\",\"searchType\": \"image\",\"dimensions\":[\"date\",\"page\"]}\n",
    "try:\n",
    "    quered_results = webmasters_service.searchanalytics().query(\n",
    "        key=\"<KEY>\",\n",
    "        siteUrl={\"insert-web-uri\"},\n",
    "        body=query_params,\n",
    "        fields=\"rows\",\n",
    "        alt=\"json\"\n",
    "    ).execute()\n",
    "    print(quered_results)\n",
    "except googleapiclient.errors.HttpError as e:\n",
    "    print(e)\n",
    "    \n",
    "##pull data for different search_types\n",
    "image_report = pd.json_normalize(quered_results, record_path=['rows'],\n",
    "    errors='ignore')\n",
    "\n",
    "image_report['search_type'] = \"image\" \n",
    "\n",
    "## create a dataframe\n",
    "image_report = pd.DataFrame(data=image_report)\n",
    "\n",
    "# new df from the column of lists\n",
    "split_df = pd.DataFrame(image_report['keys'].tolist(), columns=['date', 'landingpage'])\n",
    "# display the resulting df\n",
    "\n",
    "\n",
    "#concat previious dataframe\n",
    "image_report = pd.concat([image_report, split_df], axis=1)\n",
    "\n",
    "# drop Values\n",
    "image_report = image_report.drop('keys', axis=1)\n",
    "image_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for search_type video\n",
    "query_params = {\"startDate\": \"2022-02-25\", \"endDate\": \"2022-03-05\",\"searchType\": \"video\",\"dimensions\":[\"date\",\"page\"]}\n",
    "try:\n",
    "    quered_results = webmasters_service.searchanalytics().query(\n",
    "        key=\"<KEY>\",\n",
    "        siteUrl={\"insert-web-uri\"},\n",
    "        body=query_params,\n",
    "        fields=\"rows\",\n",
    "        alt=\"json\"\n",
    "    ).execute()\n",
    "    print(quered_results)\n",
    "except googleapiclient.errors.HttpError as e:\n",
    "    print(e)\n",
    "    \n",
    "##pull data for different search_types\n",
    "video_report = pd.json_normalize(quered_results, record_path=['rows'],\n",
    "    errors='ignore')\n",
    "\n",
    "video_report['search_type'] = \"video\" \n",
    "\n",
    "## create a dataframe\n",
    "video_report = pd.DataFrame(data=video_report)\n",
    "\n",
    "# new df from the column of lists\n",
    "split_df = pd.DataFrame(video_report['keys'].tolist(), columns=['date', 'landingpage'])\n",
    "# display the resulting df\n",
    "\n",
    "\n",
    "#concat previious dataframe\n",
    "video_report = pd.concat([video_report, split_df], axis=1)\n",
    "\n",
    "# drop Values\n",
    "video_report = video_report.drop('keys', axis=1)\n",
    "video_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for search type google_news\n",
    "query_params = {\"startDate\": \"2022-02-25\", \"endDate\": \"2022-03-05\",\"searchType\": \"google_news\",\"dimensions\":[\"date\",\"page\"]}\n",
    "try:\n",
    "    quered_results = webmasters_service.searchanalytics().query(\n",
    "        key=\"<KEY>\",\n",
    "        siteUrl={\"insert-web-uri\"},\n",
    "        body=query_params,\n",
    "        fields=\"rows\",\n",
    "        alt=\"json\"\n",
    "    ).execute()\n",
    "    print(quered_results)\n",
    "except googleapiclient.errors.HttpError as e:\n",
    "    print(e)\n",
    "    \n",
    "##pull data for different search_types\n",
    "google_news_report = pd.json_normalize(quered_results, record_path=['rows'],\n",
    "    errors='ignore')\n",
    "\n",
    "google_news_report['search_type'] = \"google_news\" \n",
    "\n",
    "## create a dataframe\n",
    "google_news_report = pd.DataFrame(data=google_news_report)\n",
    "\n",
    "# new df from the column of lists\n",
    "split_df = pd.DataFrame(google_news_report['keys'].tolist(), columns=['date', 'landingpage'])\n",
    "# display the resulting df\n",
    "\n",
    "\n",
    "#concat previious dataframe\n",
    "google_news_report = pd.concat([google_news_report, split_df], axis=1)\n",
    "\n",
    "# drop Values\n",
    "google_news_report = google_news_report.drop('keys', axis=1)\n",
    "google_news_report['position'] = \"0\" \n",
    "google_news_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ea534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for search type google_news\n",
    "query_params = {\"startDate\": \"2022-02-25\", \"endDate\": \"2022-03-05\",\"searchType\": \"discover\",\"dimensions\":[\"date\",\"page\"]}\n",
    "try:\n",
    "    quered_results = webmasters_service.searchanalytics().query(\n",
    "        key=\"<KEY>\",\n",
    "        siteUrl={\"insert-web-uri\"},\n",
    "        body=query_params,\n",
    "        fields=\"rows\",\n",
    "        alt=\"json\"\n",
    "    ).execute()\n",
    "    print(quered_results)\n",
    "except googleapiclient.errors.HttpError as e:\n",
    "    print(e)\n",
    "    \n",
    "##pull data for different search_types\n",
    "discover_report = pd.json_normalize(quered_results, record_path=['rows'],\n",
    "    errors='ignore')\n",
    "\n",
    "discover_report['search_type'] = \"discover\" \n",
    "\n",
    "## create a dataframe\n",
    "discover_report = pd.DataFrame(data=discover_report)\n",
    "\n",
    "# new df from the column of lists\n",
    "split_df = pd.DataFrame(discover_report['keys'].tolist(), columns=['date', 'landingpage'])\n",
    "# display the resulting df\n",
    "\n",
    "\n",
    "#concat previious dataframe\n",
    "discover_report = pd.concat([discover_report, split_df], axis=1)\n",
    "\n",
    "# drop Values\n",
    "discover_report = discover_report.drop('keys', axis=1)\n",
    "discover_report['position'] = \"0\" \n",
    "discover_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data for all the search types in one dataframe by concatenating them\n",
    "web_news = pd.concat([web_report, news_report])\n",
    "web_news_image = pd.concat([web_news, image_report])\n",
    "web_news_image_video = pd.concat([web_news_image, video_report])\n",
    "web_news_image_video_gn = pd.concat([web_news_image_video, google_news_report])\n",
    "merge_data_frame = pd.concat([web_news_image_video_gn, discover_report])\n",
    "merge_data_frame\n",
    "\n",
    "##you can now manipulate the dataframe as you see fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e539b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
